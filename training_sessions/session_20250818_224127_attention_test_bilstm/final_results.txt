BLSTM Training Results - attention_test_bilstm
==================================================

Configuration: configs/training/attention_training_v1.yaml
Training time: 22.6 minutes
Feature dimension: 60

Feature Configuration:
-------------------------
  Head-SSM: Enabled (12D)
    Head words: 2
  Tail-SSM: Enabled (12D)
    Tail words: 2
  Phonetic-SSM: Enabled (12D)
    Mode: rhyme
    Similarity: binary
    Normalize: False
    High sim threshold: 0.32
  POS-SSM: Enabled (12D)
    Tagset: simplified
    Similarity: combined
    High sim threshold: 0.28

Model Architecture:
------------------
  Input dimension: 60D
  Hidden dimension: 256D
  LSTM layers: 2
  ✅ Multi-layer BiLSTM architecture
      Inter-layer dropout: 0.35
      Total parameters: 2,755,842
  Output dropout: 0.18
  Batch size: 24
  Learning rate: 0.0004
  Label smoothing: 0.1
  Weighted sampling: True

Test Results:
-------------
  Macro F1: 0.7829
  Verse F1: 0.8779
  Chorus F1: 0.6880
  Confidence: 0.841
  Chorus rate: 28.31%
  Calibration: ['temperature', 'platt']

Calibration Details:
--------------------
  temperature: ECE 0.0489 → 0.0149 (Δ+0.0340)
    T = 0.735
  platt: ECE 0.0489 → 0.0199 (Δ+0.0290)
    A = 1.293, B = 0.000
