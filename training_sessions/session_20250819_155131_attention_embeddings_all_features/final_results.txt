BLSTM Training Results - attention_embeddings_all_features
==================================================

Configuration: configs/training/attention_training_v1.yaml
Training time: 43.8 minutes
Feature dimension: 84

Feature Configuration:
-------------------------
  Head-SSM: Enabled (12D)
    Head words: 2
  Tail-SSM: Enabled (12D)
    Tail words: 2
  Phonetic-SSM: Enabled (12D)
    Mode: rhyme
    Similarity: binary
    Normalize: False
    High sim threshold: 0.32
  POS-SSM: Enabled (12D)
    Tagset: simplified
    Similarity: combined
    High sim threshold: 0.28
  String-SSM: Enabled (12D)
    Case sensitive: False
    Remove punctuation: True
    Similarity threshold: 0.06
    Similarity method: word_overlap

  Word2Vec Embeddings: Enabled (12D)
    Model: word2vec-google-news-300
    Mode: summary (12D statistical features)
    Normalize: True
    Similarity metric: cosine
    High similarity threshold: 0.8
  Contextual Embeddings: Enabled (12D)
    Model: all-MiniLM-L6-v2
    Mode: summary (12D statistical features)
    Normalize: True
    Similarity metric: cosine
    High similarity threshold: 0.7

  Total Feature Dimension: 84D (Head-SSM: 12D + Tail-SSM: 12D + Phonetic-SSM: 12D + POS-SSM: 12D + String-SSM: 12D + Word2Vec: 12D + Contextual: 12D)

Model Architecture:
------------------
  Input dimension: 84D
  Hidden dimension: 192D
  LSTM layers: 2
  ✅ Multi-layer BiLSTM architecture
      Inter-layer dropout: 0.35
      Total parameters: 1,907,714
  Attention type: self
  Positional encoding: True
    PE max length: 1000
  Output dropout: 0.22
  Batch size: 32
  Learning rate: 0.0004
  Label smoothing: 0.12
  Weighted sampling: True

Test Results:
-------------
  Macro F1: 0.7713
  Verse F1: 0.8838
  Chorus F1: 0.6587
  Confidence: 0.821
  Chorus rate: 23.08%
  Calibration: ['temperature', 'platt']

Calibration Details:
--------------------
  temperature: ECE 0.0980 → 0.0099 (Δ+0.0881)
    T = 0.569
  platt: ECE 0.0980 → 0.1957 (Δ-0.0977)
    A = 0.500, B = 0.000
