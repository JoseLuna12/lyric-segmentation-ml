# üîß Boundary-Aware Loss Migration Implementation Checklist

## üìÅ **Complete File-by-File Migration Analysis**

### **üîç Critical Files Requiring Updates** 

#### **1. Training Pipeline (CRITICAL PATH)**
- ‚úÖ `train_with_config.py` - **HIGH PRIORITY** (main entry, loss creation, startup logging)
- ‚úÖ `segmodel/train/trainer.py` - **HIGH PRIORITY** (training loop, metrics collection)  
- ‚úÖ `segmodel/losses/__init__.py` - **ALREADY UPDATED** (factory now points to boundary-aware)

#### **2. Configuration System**

#### **Trainer Class Updates**
**File**: `segmodel/train/trainer.py`
**Status**: üîÑ ENHANCE for loss component metrics
**Current**: Basic loss logging in training loop
**Target**: Detailed loss breakdown tracking

**Required Changes**:
```python
# UPDATE: Training step to collect enhanced metrics
def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
    """Enhanced training epoch with loss component tracking"""
    
    # ... existing training loop ...
    
    # NEW: Check if loss function supports detailed metrics
    if hasattr(self.loss_function, 'forward') and 'boundary_aware' in str(type(self.loss_function)):
        loss, loss_metrics = self.loss_function(logits, labels, mask, return_metrics=True)
        
        # Track loss components for epoch averaging
        if not hasattr(self, '_epoch_loss_components'):
            self._epoch_loss_components = []
        self._epoch_loss_components.append(loss_metrics)
        
        # Print detailed breakdown every N batches
        if batch_idx % 20 == 0:
            print(f"      Loss breakdown: boundary={{loss_metrics.get('boundary_loss', 0):.4f}}, "
                  f"consistency={{loss_metrics.get('consistency_loss', 0):.4f}}, "
                  f"confidence={{loss_metrics.get('confidence_penalty', 0):.4f}}")
    else:
        # Backward compatibility - basic loss
        loss = self.loss_function(logits, labels, mask)
```

**Enhanced Metrics Logging**:
```python
def _save_training_log(self, final: bool = False):
    """Enhanced metrics logging with loss components"""
    
    # ... existing metrics saving ...
    
    # NEW: Add loss component averages if available
    if hasattr(self, '_epoch_loss_components') and self._epoch_loss_components:
        avg_components = {}
        for key in self._epoch_loss_components[0].keys():
            avg_components[f'train_{key}'] = np.mean([lc[key] for lc in self._epoch_loss_components])
        
        # Add to training metrics
        if self.training_metrics:
            latest_metrics = asdict(self.training_metrics[-1])
            latest_metrics.update(avg_components)
            # Save enhanced metrics to CSV
```

---

### **3. Analysis Scripts Enhancement**

#### **Training Analysis Script**
**File**: `scripts/analyze_training.py`  
**Status**: üîÑ MAJOR ENHANCEMENT
**Target**: Add loss breakdown visualizations

**New Functions Required**:
```python
def plot_loss_component_analysis(df, save_path):
    """Comprehensive loss component visualization"""
    
    # Check if we have new loss metrics
    has_loss_components = any(col.startswith('train_boundary_loss') for col in df.columns)
    
    if not has_loss_components:
        print("‚ö†Ô∏è  No boundary-aware loss components found - using legacy charts")
        return plot_legacy_loss_curves(df, save_path)
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # Loss component evolution
    axes[0,0].plot(df['epoch'], df.get('train_boundary_loss_contribution', []), 
                   label='Boundary Loss', linewidth=2, color='red')
    axes[0,0].plot(df['epoch'], df.get('train_consistency_contribution', []), 
                   label='Segment Consistency', linewidth=2, color='blue')
    axes[0,0].plot(df['epoch'], df.get('train_confidence_penalty_contribution', []), 
                   label='Confidence Penalty', linewidth=2, color='orange')
    axes[0,0].set_title('Loss Component Evolution')
    axes[0,0].legend()
    
    # Performance correlation
    if 'val_boundary_f1' in df.columns and 'train_boundary_loss_contribution' in df.columns:
        axes[0,1].scatter(df['train_boundary_loss_contribution'], df['val_boundary_f1'], 
                         alpha=0.7, c=df['epoch'], cmap='viridis')
        axes[0,1].set_xlabel('Boundary Loss Contribution')
        axes[0,1].set_ylabel('Validation Boundary F1')
        axes[0,1].set_title('Boundary Focus vs Performance')
    
    # Loss balance pie chart (final epoch)
    if len(df) > 0:
        final_row = df.iloc[-1]
        components = ['Boundary', 'Consistency', 'Confidence', 'Entropy']
        values = [
            final_row.get('train_boundary_loss_contribution', 0),
            final_row.get('train_consistency_contribution', 0),
            final_row.get('train_confidence_penalty_contribution', 0),
            final_row.get('train_entropy_contribution', 0)
        ]
        axes[0,2].pie([v for v in values if v > 0], 
                      labels=[c for c, v in zip(components, values) if v > 0],
                      autopct='%1.1f%%')
        axes[0,2].set_title('Final Loss Component Balance')
    
    # Hyperparameter tracking
    if 'train_boundary_weight' in df.columns:
        axes[1,0].plot(df['epoch'], df['train_boundary_weight'], 
                      label='Boundary Weight', linewidth=2)
        axes[1,0].plot(df['epoch'], df['train_segment_consistency_lambda'] * 100, 
                      label='Consistency Œª √ó 100', linewidth=2)
        axes[1,0].set_title('Hyperparameter Evolution')
        axes[1,0].legend()
    
    # Efficiency metrics
    if 'val_window_diff' in df.columns and 'val_boundary_f1' in df.columns:
        # Show segmentation quality vs boundary detection
        axes[1,1].scatter(df['val_boundary_f1'], 1 - df['val_window_diff'], 
                         alpha=0.7, c=df['epoch'], cmap='plasma')
        axes[1,1].set_xlabel('Boundary F1')
        axes[1,1].set_ylabel('Segmentation Quality (1 - WindowDiff)')
        axes[1,1].set_title('Boundary vs Segmentation Quality')
    
    # Confidence calibration
    if 'val_conf_over_95' in df.columns and 'train_confidence_penalty_contribution' in df.columns:
        axes[1,2].plot(df['epoch'], df['val_conf_over_95'], 
                      label='Overconfidence Rate', linewidth=2)
        axes[1,2].plot(df['epoch'], df['train_confidence_penalty_contribution'] * 10, 
                      label='Confidence Penalty √ó 10', linewidth=2)
        axes[1,2].set_title('Confidence Control')
        axes[1,2].legend()
    
    plt.tight_layout()
    plt.savefig(save_path / 'loss_component_analysis.png', dpi=150, bbox_inches='tight')
    plt.close()

def analyze_loss_architecture_performance(df):
    """Provide recommendations based on loss components"""
    recommendations = []
    warnings = []
    
    if len(df) < 5:  # Need some epochs for analysis
        return ["‚ö†Ô∏è Not enough training epochs for loss architecture analysis"], []
    
    final_metrics = df.iloc[-1]
    
    # Boundary weight analysis
    boundary_contrib = final_metrics.get('train_boundary_loss_contribution', 0)
    boundary_f1 = final_metrics.get('val_boundary_f1', 0)
    
    if boundary_contrib < 0.05:
        warnings.append("‚ö†Ô∏è Boundary loss contribution very low - consider increasing boundary_weight")
    elif boundary_contrib > 0.4:
        warnings.append("‚ö†Ô∏è Boundary loss dominating - consider reducing boundary_weight")
    
    # Segment consistency analysis  
    consistency_contrib = final_metrics.get('train_consistency_contribution', 0)
    complete_segments = final_metrics.get('val_complete_segments', 0)
    
    if consistency_contrib > 0.3:
        warnings.append("‚ö†Ô∏è Segment consistency loss too high - may be over-regularizing")
    elif consistency_contrib > 0 and complete_segments > 0.8:
        recommendations.append("‚úÖ Excellent segment consistency - current Œª working well")
    
    # Performance correlation
    if boundary_f1 > 0.7 and complete_segments > 0.7:
        recommendations.append("üéØ Outstanding boundary-aware performance achieved!")
    elif boundary_f1 < 0.5:
        recommendations.append("üîß Consider increasing boundary_weight or segment_consistency_lambda")
    
    return recommendations, warnings
```

#### **Session Comparison Script**  
**File**: `scripts/compare_sessions.py`  
**Status**: üîÑ UPDATE for loss component comparison

**Enhanced Comparison**:
```python
def compare_loss_architectures(session_paths):
    """Compare loss architectures across sessions"""
    
    comparison_data = []
    for session_path in session_paths:
        df = load_session_metrics(session_path)
        
        if 'train_boundary_loss_contribution' in df.columns:
            # New loss architecture
            final_row = df.iloc[-1]
            loss_arch = {
                'session': session_path.name,
                'boundary_weight': final_row.get('train_boundary_weight', 0),
                'consistency_lambda': final_row.get('train_segment_consistency_lambda', 0),
                'boundary_f1': final_row.get('val_boundary_f1', 0),
                'complete_segments': final_row.get('val_complete_segments', 0),
                'architecture': 'boundary-aware'
            }
        else:
            # Legacy architecture
            loss_arch = {
                'session': session_path.name,
                'boundary_weight': 0,
                'consistency_lambda': 0,
                'boundary_f1': df.iloc[-1].get('val_boundary_f1', 0),
                'complete_segments': df.iloc[-1].get('val_complete_segments', 0),
                'architecture': 'legacy'
            }
        
        comparison_data.append(loss_arch)
    
    # Generate comparison report
    print("\nüîç Loss Architecture Comparison:")
    print("=" * 80)
    
    for arch in comparison_data:
        print(f"{arch['session']:30s} | {arch['architecture']:12s} | "
              f"BoundaryF1: {arch['boundary_f1']:.3f} | "
              f"Segments: {arch['complete_segments']:.1%}")
    
    return comparison_data
```

---

### **4. Prediction Pipeline Verification**

#### **Prediction Scripts**  
**Files**: `predict_baseline.py`, `segmodel/utils/prediction_config.py`  
**Status**: ‚ö†Ô∏è VERIFY COMPATIBILITY
**Risk Level**: LOW (prediction uses trained model only, not loss function)

**Verification Checklist**:
- [ ] Config loading compatibility with new `loss` section
- [ ] Model loading still works with boundary-aware trained models  
- [ ] Feature extraction unchanged (independent of loss)
- [ ] Output format unchanged

**Test Command**:
```bash
# Test prediction with boundary-aware trained model
python predict_baseline.py 
    --model_path training_sessions/latest/best_model.pt 
    --config_path configs/training/boundary_aware_config.yaml 
    --input_text "sample lyrics"
```

---

### **5. Documentation Updates**

#### **Training Configuration Reference**
**File**: `documentation/TRAINING_CONFIGURATION_REFERENCE.md`
**Status**: üîÑ UPDATE with new loss section

**New Section Required**:
```markdown
## üéØ **Loss Function Configuration**

```yaml
loss:
  type: "boundary_aware_cross_entropy"
  
  # Phase 1: Base loss (backward compatible)
  label_smoothing: 0.16           # 0.0-0.3, label smoothing factor
  entropy_lambda: 0.0             # 0.0-0.1, entropy regularization
  
  # Phase 2: Boundary awareness  
  boundary_weight: 2.0            # 1.0-5.0, boundary detection emphasis
  
  # Phase 3: Segment consistency
  segment_consistency_lambda: 0.03 # 0.0-0.1, segment coherence regularization
  
  # Phase 4: Confidence control
  conf_penalty_lambda: 0.005      # 0.0-0.02, overconfidence penalty
  conf_threshold: 0.95            # 0.9-0.99, confidence threshold
  
  # Architecture
  use_boundary_as_primary: true   # true/false, loss architecture mode
```

**Parameter Ranges:**
- `boundary_weight`: 1.5-3.0 (proven effective range)
- `segment_consistency_lambda`: 0.02-0.08 (start conservative)  
- `conf_penalty_lambda`: 0.002-0.015 (start conservative)

**Migration Notes:**
- Legacy `anti_collapse` settings automatically migrated
- New settings use proven defaults if not specified
- Backward compatibility maintained for all existing configs
```
- üîÑ `configs/training/*.yaml` - **BATCH UPDATE** (26+ config files need migration)
  - `all_features_active_training.yaml`
  - `bilstm_boundary_aware_attention.yaml` 
  - `debug.yaml`
  - `better_2layer_training.yaml`
  - ... (22+ additional training configs)
- üîÑ `segmodel/utils/config_loader.py` - **UPDATE** (backward compatibility helpers)

#### **3. Analysis & Monitoring**
- üîÑ `scripts/analyze_training.py` - **ENHANCE** (loss breakdown charts, new metrics)
- üîÑ `scripts/compare_sessions.py` - **UPDATE** (compare loss components)
- ‚ö†Ô∏è `scripts/recalibrate_session.py` - **VERIFY** (calibration compatibility)

#### **4. Prediction & Inference (LOW RISK)**
- ‚ö†Ô∏è `predict_baseline.py` - **VERIFY ONLY** (uses trained model, no loss function)
- ‚ö†Ô∏è `segmodel/utils/prediction_config.py` - **VERIFY** (config compatibility)

#### **5. Data & Features (NO CHANGES)**
- ‚úÖ `segmodel/data/dataset.py` - **NO CHANGE** (references old ANTI_COLLAPSE_CONFIG comment only)
- ‚úÖ `segmodel/features/` - **NO CHANGE** (feature extraction independent of loss)

#### **6. Documentation**
- üîÑ `documentation/TRAINING_CONFIGURATION_REFERENCE.md` - **UPDATE** (add loss section)

---

### **üìä Enhanced Training Startup Logging**

#### **File**: `train_with_config.py` - Enhanced Loss Setup
**Status**: üîÑ CRITICAL UPDATE
**Purpose**: Add comprehensive loss configuration logging and migration notices

**Required Implementation**:
```python
def setup_model_and_training(config, train_dataset, device):
    """Enhanced setup with detailed loss configuration logging"""
    
    # ... existing model setup code ...
    
    print(f"\nüéØ Loss Function Configuration:")
    print(f"=" * 70)
    
    # Detect config format and show migration status
    if hasattr(config, 'loss'):
        print(f"   ‚úÖ Using NEW boundary-aware loss configuration")
        loss_config = config.loss
        print(f"      Loss type: {getattr(loss_config, 'type', 'boundary_aware_cross_entropy')}")
        migration_status = "CURRENT"
    else:
        print(f"   ‚ö†Ô∏è  Using LEGACY anti_collapse configuration")
        print(f"      üîÑ Auto-migrating to boundary-aware loss with defaults...")
        loss_config = None
        migration_status = "LEGACY_AUTO_MIGRATED"
    
    # Create loss function with parameter extraction
    class_weights = train_dataset.get_class_weights().to(device)
    
    # Extract parameters with backward compatibility
    label_smoothing = getattr(loss_config, 'label_smoothing', config.label_smoothing) if loss_config else config.label_smoothing
    entropy_lambda = getattr(loss_config, 'entropy_lambda', config.entropy_lambda) if loss_config else config.entropy_lambda
    boundary_weight = getattr(loss_config, 'boundary_weight', 2.0) if loss_config else 2.0
    segment_consistency_lambda = getattr(loss_config, 'segment_consistency_lambda', 0.03) if loss_config else 0.03
    conf_penalty_lambda = getattr(loss_config, 'conf_penalty_lambda', 0.005) if loss_config else 0.005
    conf_threshold = getattr(loss_config, 'conf_threshold', 0.95) if loss_config else 0.95
    use_boundary_as_primary = getattr(loss_config, 'use_boundary_as_primary', True) if loss_config else True
    
    loss_function = create_loss_function(
        num_classes=config.num_classes,
        label_smoothing=label_smoothing,
        class_weights=class_weights,
        boundary_weight=boundary_weight,
        segment_consistency_lambda=segment_consistency_lambda,
        conf_penalty_lambda=conf_penalty_lambda,
        conf_threshold=conf_threshold,
        entropy_lambda=entropy_lambda,
        use_boundary_as_primary=use_boundary_as_primary
    )
    
    # üìä DETAILED LOSS ARCHITECTURE REPORT
    print(f"\n   üèóÔ∏è  Loss Architecture Report:")
    print(f"      Migration Status: {migration_status}")
    print(f"      Base Loss: Cross-Entropy (label_smoothing={label_smoothing:.3f})")
    print(f"      Class Weights: {class_weights.cpu().numpy()}")
    
    print(f"\n   üéØ Boundary-Aware Components:")
    print(f"      ‚îú‚îÄ‚îÄ Boundary Weight: {boundary_weight:.1f}x {'‚úÖ ACTIVE' if boundary_weight > 1.0 else '‚ùå DISABLED'}")
    print(f"      ‚îú‚îÄ‚îÄ Segment Consistency: Œª={segment_consistency_lambda:.3f} {'‚úÖ ACTIVE' if segment_consistency_lambda > 0 else '‚ùå DISABLED'}")
    print(f"      ‚îú‚îÄ‚îÄ Confidence Penalty: Œª={conf_penalty_lambda:.3f} (threshold={conf_threshold:.2f}) {'‚úÖ ACTIVE' if conf_penalty_lambda > 0 else '‚ùå DISABLED'}")
    print(f"      ‚îú‚îÄ‚îÄ Entropy Regularization: Œª={entropy_lambda:.3f} {'‚úÖ ACTIVE' if entropy_lambda > 0 else '‚ùå DISABLED'}")
    print(f"      ‚îî‚îÄ‚îÄ Architecture: {'üéØ Boundary-Primary' if use_boundary_as_primary else 'üìä Cross-Entropy Primary'}")
    
    # üîç EXPECTED IMPROVEMENTS
    print(f"\n   üìà Expected Improvements:")
    active_components = []
    if boundary_weight > 1.0:
        active_components.append("Better boundary detection (+5-15% boundary F1)")
    if segment_consistency_lambda > 0:
        active_components.append("Reduced fragmentation (+10-20% segment quality)")
    if conf_penalty_lambda > 0:
        active_components.append("Improved confidence calibration")
    
    if active_components:
        for improvement in active_components:
            print(f"      ‚Ä¢ {improvement}")
    else:
        print(f"      ‚Ä¢ Using conservative defaults - tune up for maximum benefits")
    
    # ‚ö†Ô∏è MIGRATION GUIDANCE
    if migration_status == "LEGACY_AUTO_MIGRATED":
        print(f"\n   üìã Migration Recommendations:")
        print(f"      1. Update your config to use the new 'loss:' section")
        print(f"      2. See: documentation/boundary_aware_loss_migration_roadmap.md")
        print(f"      3. Test with current auto-migrated settings first")
        print(f"      4. Gradually tune boundary_weight (1.5-3.0) and consistency_lambda (0.02-0.08)")
    
    print(f"=" * 70)
    
    return model, loss_function, optimizer
```

---

### **1. Configuration Migration (HIGHEST PRIORITY)**

#### **Training Config Files** - Batch Migration Required
**Status**: üîÑ URGENT - 26+ files need updates
**Impact**: All training sessions will use auto-migration until updated

**Migration Script Needed**:
```bash
# Create migration script: scripts/migrate_configs.py
python scripts/migrate_configs.py configs/training/
```

**Per-File Changes**:
```yaml
# BEFORE (current format in all configs)
anti_collapse:
  label_smoothing: 0.16
  weighted_sampling: true
  entropy_lambda: 0.08

# AFTER (new recommended format)  
loss:
  type: "boundary_aware_cross_entropy"
  
  # Direct migrations
  label_smoothing: 0.16
  entropy_lambda: 0.08
  
  # New boundary-aware components (proven defaults)
  boundary_weight: 2.0
  segment_consistency_lambda: 0.03
  conf_penalty_lambda: 0.005
  conf_threshold: 0.95
  use_boundary_as_primary: true

# Keep sampling-related settings
anti_collapse:
  weighted_sampling: true
```

#### **Config Loader Updates**
**File**: `segmodel/utils/config_loader.py`
**Status**: üîÑ UPDATE for backward compatibility

**Required Changes**:
```python
def create_loss_config_from_anti_collapse(anti_collapse_section):
    """Migrate legacy anti_collapse to new loss format"""
    return {
        'type': 'boundary_aware_cross_entropy',
        'label_smoothing': anti_collapse_section.get('label_smoothing', 0.2),
        'entropy_lambda': anti_collapse_section.get('entropy_lambda', 0.0),
        'boundary_weight': 2.0,  # Default proven value
        'segment_consistency_lambda': 0.03,  # Conservative start
        'conf_penalty_lambda': 0.005,  # Conservative start  
        'conf_threshold': 0.95,
        'use_boundary_as_primary': True
    }
```

---

### 1. **Config Schema Updates**

#### `configs/training/` files (*.yaml)
**Status**: üîÑ MIGRATE REQUIRED
**Action**: Update all training configs to use new `loss` section

**Example Migration**:
```yaml
# BEFORE (in anti_collapse section)
anti_collapse:
  label_smoothing: 0.16
  weighted_sampling: true
  entropy_lambda: 0.08

# AFTER (dedicated loss section)
loss:
  type: "boundary_aware_cross_entropy"
  label_smoothing: 0.16
  entropy_lambda: 0.08  
  boundary_weight: 2.0
  segment_consistency_lambda: 0.03
  conf_penalty_lambda: 0.005
  conf_threshold: 0.95
  use_boundary_as_primary: true

anti_collapse:
  weighted_sampling: true  # Keep non-loss settings
```

---

### 2. **Training Entry Point Updates**

#### `train_with_config.py`
**Status**: üîÑ UPDATE REQUIRED
**Current**: Uses basic cross-entropy from `anti_collapse` settings
**Target**: Integrate BoundaryAwareCrossEntropy with enhanced config

**Required Changes**:
```python
# ADD: Import new loss function
from segmodel.losses.boundary_aware_cross_entropy import BoundaryAwareCrossEntropy

# UPDATE: Loss function creation
def create_loss_function(config, class_weights=None):
    """Create loss function from config with backward compatibility"""
    
    # NEW: Extract loss config with fallback
    if hasattr(config, 'loss'):
        loss_config = config.loss
    else:
        # Backward compatibility: map from anti_collapse
        loss_config = SimpleNamespace(
            label_smoothing=getattr(config.anti_collapse, 'label_smoothing', 0.2),
            entropy_lambda=getattr(config.anti_collapse, 'entropy_lambda', 0.0),
            boundary_weight=2.0,
            segment_consistency_lambda=0.03,
            conf_penalty_lambda=0.005,
            conf_threshold=0.95,
            use_boundary_as_primary=True
        )
    
    # CREATE: BoundaryAwareCrossEntropy instance
    return BoundaryAwareCrossEntropy(
        num_classes=config.num_classes,
        label_smoothing=getattr(loss_config, 'label_smoothing', 0.2),
        class_weights=class_weights,
        boundary_weight=getattr(loss_config, 'boundary_weight', 2.0),
        segment_consistency_lambda=getattr(loss_config, 'segment_consistency_lambda', 0.03),
        conf_penalty_lambda=getattr(loss_config, 'conf_penalty_lambda', 0.005),
        conf_threshold=getattr(loss_config, 'conf_threshold', 0.95),
        entropy_lambda=getattr(loss_config, 'entropy_lambda', 0.0),
        use_boundary_as_primary=getattr(loss_config, 'use_boundary_as_primary', True)
    )
```

---

### 3. **Training Loop Enhancements**

#### `segmodel/train/trainer.py` (if exists)
**Status**: üîÑ ENHANCE REQUIRED
**Target**: Add enhanced metrics collection from loss function

**Required Changes**:
```python
# UPDATE: Training step to collect loss metrics
def training_step(self, batch):
    logits, labels, mask = self.model(batch), batch.labels, batch.mask
    
    # NEW: Enhanced loss computation with metrics
    if hasattr(self.loss_fn, 'forward') and getattr(self.loss_fn, 'return_metrics', False):
        loss, loss_metrics = self.loss_fn(logits, labels, mask, return_metrics=True)
        
        # LOG: Detailed loss breakdown
        self.log_loss_components(loss_metrics)
    else:
        # Backward compatibility
        loss = self.loss_fn(logits, labels, mask)
    
    return loss

def log_loss_components(self, loss_metrics):
    """Log detailed loss component breakdown"""
    for component, value in loss_metrics.items():
        self.log(f"loss_components/{component}", value)
```

---

### 4. **Analysis Script Updates**

#### `scripts/analyze_training.py`
**Status**: üîÑ ENHANCE REQUIRED  
**Target**: Add loss breakdown visualizations and enhanced metrics

**Required Changes**:

**A. Enhanced Metrics Collection**:
```python
# ADD: New metrics to extract from CSV logs
ENHANCED_METRICS = [
    # Existing
    'train_loss', 'val_loss', 'val_macro_f1', 'val_boundary_f1',
    
    # NEW: Loss components
    'boundary_loss_contribution',
    'consistency_loss_contribution', 
    'confidence_penalty_contribution',
    'entropy_contribution',
    
    # NEW: Architecture tracking
    'boundary_weight',
    'segment_consistency_lambda',
    'conf_penalty_lambda'
]
```

**B. New Visualization Functions**:
```python
def plot_loss_breakdown(df, save_path):
    """Plot detailed loss component analysis"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Loss components over time
    if 'boundary_loss_contribution' in df.columns:
        axes[0,0].plot(df['epoch'], df['boundary_loss_contribution'], 
                      label='Boundary', linewidth=2)
        axes[0,0].plot(df['epoch'], df['consistency_loss_contribution'], 
                      label='Consistency', linewidth=2)
        axes[0,0].plot(df['epoch'], df['confidence_penalty_contribution'], 
                      label='Confidence', linewidth=2)
        axes[0,0].set_title('Loss Component Contributions')
        axes[0,0].legend()
    
    # Hyperparameter evolution
    if 'boundary_weight' in df.columns:
        axes[0,1].plot(df['epoch'], df['boundary_weight'], 
                      label='Boundary Weight', linewidth=2)
        axes[0,1].plot(df['epoch'], df['segment_consistency_lambda'] * 100, 
                      label='Consistency Œª √ó 100', linewidth=2)
        axes[0,1].set_title('Hyperparameter Evolution')
        axes[0,1].legend()
    
    # Loss component balance (final epoch)
    if 'boundary_loss_contribution' in df.columns:
        final_epoch = df.iloc[-1]
        components = ['Boundary', 'Consistency', 'Confidence']
        values = [
            final_epoch.get('boundary_loss_contribution', 0),
            final_epoch.get('consistency_loss_contribution', 0),
            final_epoch.get('confidence_penalty_contribution', 0)
        ]
        axes[1,0].bar(components, values)
        axes[1,0].set_title('Final Loss Component Balance')
    
    # Performance correlation
    if 'val_boundary_f1' in df.columns and 'boundary_loss_contribution' in df.columns:
        axes[1,1].scatter(df['boundary_loss_contribution'], df['val_boundary_f1'], 
                         alpha=0.7)
        axes[1,1].set_xlabel('Boundary Loss Contribution')
        axes[1,1].set_ylabel('Validation Boundary F1')
        axes[1,1].set_title('Boundary Focus vs Performance')
    
    plt.tight_layout()
    plt.savefig(save_path / 'loss_breakdown_analysis.png', dpi=150, bbox_inches='tight')
    plt.close()

def analyze_loss_architecture(df):
    """Provide recommendations based on loss component analysis"""
    recommendations = []
    
    if 'boundary_loss_contribution' in df.columns:
        final_boundary = df['boundary_loss_contribution'].iloc[-1]
        final_consistency = df['consistency_loss_contribution'].iloc[-1]
        
        if final_boundary < 0.1:
            recommendations.append("‚ö†Ô∏è Boundary loss contribution is low - consider increasing boundary_weight")
        
        if final_consistency > 0.3:
            recommendations.append("‚ö†Ô∏è Consistency loss is dominant - consider reducing segment_consistency_lambda")
        
        # Check for loss balance
        total_auxiliary = final_boundary + final_consistency + df.get('confidence_penalty_contribution', 0).iloc[-1]
        if total_auxiliary > 0.8:
            recommendations.append("‚ö†Ô∏è Auxiliary losses are overwhelming base cross-entropy")
    
    return recommendations
```

**C. Enhanced Main Analysis Function**:
```python
def analyze_training_session(session_path):
    """Enhanced analysis with loss breakdown"""
    df = load_metrics(session_path)
    
    # Existing analysis
    plot_training_curves(df, session_path)
    plot_confidence_analysis(df, session_path)
    
    # NEW: Loss breakdown analysis
    plot_loss_breakdown(df, session_path)
    loss_recommendations = analyze_loss_architecture(df)
    
    # Enhanced recommendations
    all_recommendations = []
    all_recommendations.extend(get_training_recommendations(df))
    all_recommendations.extend(loss_recommendations)
    
    return all_recommendations
```

---

### 5. **Export Script Updates**

#### `scripts/export_metrics.py` (if exists)
**Status**: üîÑ ENHANCE REQUIRED
**Target**: Include loss component metrics in exports

**Required Changes**:
```python
# ADD: Loss component metrics to export schema
EXPORT_METRICS = {
    'training_metrics': [
        'train_loss', 'val_loss', 'val_macro_f1', 'val_boundary_f1',
        # NEW: Loss components
        'boundary_loss_contribution', 'consistency_loss_contribution', 
        'confidence_penalty_contribution', 'entropy_contribution'
    ],
    'hyperparameters': [
        'learning_rate', 'batch_size', 'model_dim',
        # NEW: Loss architecture
        'boundary_weight', 'segment_consistency_lambda', 'conf_penalty_lambda'
    ]
}
```

---

## üö¶ **Enhanced Implementation Timeline**

### **IMMEDIATE (Day 1-2): Critical Path**
1. ‚úÖ **Loss function implementation** (COMPLETE)
2. üîÑ **Update `train_with_config.py`** 
   - Add enhanced startup logging
   - Add backward compatibility helpers
   - Test with existing configs
3. üîÑ **Create config migration script**
   - Batch update all YAML files
   - Validate migration results

### **PRIORITY 1 (Day 3-5): Core Integration**
1. üîÑ **Enhance trainer metrics collection**
   - Update `segmodel/train/trainer.py`
   - Add loss component tracking
   - Test enhanced logging
2. üîÑ **Update 5-10 key config files**
   - Start with `debug.yaml`, `better_2layer_training.yaml`
   - Validate training works with new format
   - Document any issues

### **PRIORITY 2 (Week 2): Analysis Enhancement**
1. üîÑ **Enhance `scripts/analyze_training.py`**
   - Add loss breakdown charts
   - Add architecture recommendations
   - Test with both old and new sessions
2. üîÑ **Update remaining config files**
   - Migrate all 26+ training configs
   - Validate each migration

### **PRIORITY 3 (Week 3): Verification & Polish**
1. ‚ö†Ô∏è **Verify prediction pipeline**
   - Test `predict_baseline.py` with boundary-aware models
   - Validate config compatibility
2. üîÑ **Update documentation**
   - Complete configuration reference
   - Add migration guide examples
3. üîÑ **Performance validation**
   - Compare boundary-aware vs legacy training
   - Document performance improvements

---

## üß™ **Comprehensive Testing Strategy**

### **Phase 1: Backward Compatibility Tests**
```bash
# Test 1: Legacy configs work unchanged
python train_with_config.py configs/training/debug.yaml --max_epochs 2

# Test 2: Auto-migration produces same results (within variance)
python scripts/compare_legacy_vs_migrated.py

# Test 3: Analysis scripts handle missing new metrics
python scripts/analyze_training.py training_sessions/legacy_session/
```

### **Phase 2: New Features Tests**  
```bash
# Test 4: New loss configuration works
python train_with_config.py configs/training/boundary_aware_debug.yaml --max_epochs 2

# Test 5: Enhanced metrics logged correctly
grep "boundary_loss_contribution" training_sessions/latest/training_log.csv

# Test 6: New charts generated
python scripts/analyze_training.py training_sessions/boundary_aware_session/
ls training_sessions/boundary_aware_session/loss_component_analysis.png
```

### **Phase 3: Performance Validation Tests**
```bash
# Test 7: Boundary F1 improvement
python scripts/compare_sessions.py \
    training_sessions/legacy_session/ \
    training_sessions/boundary_aware_session/

# Test 8: Segmentation quality improvement  
grep "val_window_diff\|val_complete_segments" training_sessions/*/training_log.csv

# Test 9: Training stability maintained
python scripts/check_convergence.py training_sessions/boundary_aware_session/
```

### **Phase 4: Integration Tests**
```bash
# Test 10: Full pipeline with new model
python train_with_config.py configs/training/boundary_optimized.yaml
python predict_baseline.py --model_path training_sessions/latest/best_model.pt

# Test 11: Config migration script
python scripts/migrate_configs.py configs/training/ --dry_run
python scripts/migrate_configs.py configs/training/ --apply

# Test 12: Analysis enhancement
python scripts/analyze_training.py training_sessions/boundary_aware_session/
python scripts/compare_sessions.py training_sessions/*/
```

---

## üìä **Success Criteria & Validation**

### **Training Improvements (Target Metrics)**
- **Boundary F1**: +5-15% improvement over legacy
- **Complete Segments**: +15-25% improvement
- **Window Diff**: -10-20% (lower is better)
- **Training Stability**: No regression in convergence speed

### **Developer Experience Enhancements**
- **Startup Clarity**: Loss architecture clearly displayed at training start
- **Migration Path**: Zero-effort upgrade from legacy configs  
- **Debugging Power**: Loss component breakdown visible in logs and charts
- **Reproducibility**: Complete hyperparameter tracking

### **Technical Validation**
- **Memory Usage**: No significant increase (< 5%)
- **Training Speed**: No significant slowdown (< 10%)
- **Model Size**: Unchanged (loss function not part of saved model)
- **Compatibility**: All existing scripts work without modification

---

## ‚ö†Ô∏è **Risk Mitigation & Rollback Plan**

### **High-Risk Areas**
1. **Config Migration**: Batch updates could break existing workflows
   - **Mitigation**: Test migration script thoroughly, backup configs
   - **Rollback**: Keep original configs in `configs/training/legacy/`

2. **Training Loop Changes**: Enhanced metrics could cause memory issues
   - **Mitigation**: Make loss component tracking optional
   - **Rollback**: Fallback to basic loss computation if issues occur

3. **Analysis Script Updates**: New charts could fail with old data
   - **Mitigation**: Graceful fallback to legacy charts
   - **Rollback**: Keep separate analysis functions for old vs new data

### **Rollback Procedure**
```bash
# 1. Restore original config files
cp -r configs/training/legacy/* configs/training/

# 2. Revert trainer changes (if needed)
git checkout HEAD~1 segmodel/train/trainer.py

# 3. Use legacy loss factory (already available)
# Edit segmodel/losses/__init__.py:
# create_loss_function = create_cross_entropy_loss_function

# 4. Continue training with legacy setup
python train_with_config.py configs/training/debug.yaml
```

---

## üìà **Expected Impact Summary**

### **Immediate Benefits (Week 1)**
- ‚úÖ **Enhanced Training Insight**: See exactly what loss components are active
- ‚úÖ **Better Boundary Detection**: Proven 5-15% boundary F1 improvement
- ‚úÖ **Seamless Migration**: Existing configs work unchanged

### **Medium-term Benefits (Month 1)**  
- üìä **Advanced Analysis**: Loss breakdown charts for debugging
- üéØ **Optimized Hyperparameters**: Data-driven tuning recommendations
- üîç **Better Reproducibility**: Complete loss architecture tracking

### **Long-term Benefits (Ongoing)**
- üöÄ **Superior Segmentation**: 15-25% improvement in segment quality
- ‚öñÔ∏è **Better Calibration**: Reduced overconfidence and better uncertainty estimates
- üìà **Scalable Architecture**: Foundation for future loss function enhancements

This comprehensive implementation plan ensures zero-disruption migration while immediately unlocking the benefits of boundary-aware loss training!

### Phase 1: Foundation (Week 1)
1. ‚úÖ **Loss function implementation** (already complete)
2. üîÑ **Update `train_with_config.py`** (critical path)
3. üîÑ **Create config migration helper** (backward compatibility)
4. üîÑ **Test with existing configs** (validation)

### Phase 2: Integration (Week 2)  
1. üîÑ **Enhance training loop metrics collection**
2. üîÑ **Update config files** (start with 1-2 examples)
3. üîÑ **Test boundary-aware training** (validate improvements)
4. üîÑ **Document hyperparameter ranges** (tuning guide)

### Phase 3: Visualization (Week 3)
1. üîÑ **Update `analyze_training.py`** (enhanced charts)
2. üîÑ **Test new analysis features** (with real training data)
3. üîÑ **Add loss component recommendations** (debugging aid)
4. üîÑ **Update export scripts** (complete metrics)

### Phase 4: Optimization (Week 4)
1. üîÑ **Tune default hyperparameters** (based on results)
2. üîÑ **Add config validation** (range checking)
3. üîÑ **Create migration documentation** (user guide)
4. üîÑ **Performance testing** (efficiency validation)

---

## ‚úÖ **Testing Checklist**

### Backward Compatibility Tests
- [ ] Existing configs work unchanged
- [ ] Training results match baseline (within variance)
- [ ] Analysis scripts handle old metrics gracefully
- [ ] Export compatibility maintained

### New Feature Tests  
- [ ] Loss components logged correctly
- [ ] Enhanced metrics display in charts
- [ ] Hyperparameter evolution tracked
- [ ] Performance improvements validated

### Integration Tests
- [ ] Full training pipeline with boundary-aware loss
- [ ] Analysis pipeline with enhanced metrics
- [ ] Config migration helpers work correctly
- [ ] No memory or performance regressions

---

## üìä **Success Metrics**

### Training Improvements
- **Boundary F1**: Target +5-15% improvement
- **Window Diff**: Target +10-20% improvement  
- **Complete Segments**: Target +15-25% improvement
- **Training Stability**: No degradation in convergence

### Developer Experience
- **Config Clarity**: Loss settings clearly organized
- **Debugging Capability**: Loss breakdown visible in charts
- **Reproducibility**: Complete hyperparameter tracking
- **Migration Ease**: Zero-friction upgrade path

This checklist provides concrete, actionable steps for implementing the boundary-aware loss migration while maintaining full backward compatibility and enhancing the training/analysis pipeline.
