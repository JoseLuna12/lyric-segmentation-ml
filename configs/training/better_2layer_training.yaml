# Enhanced BiLSTM Configuration for 2-Layer Verse-Chorus Segmentation
# Optimized based on successful 0.79+ F1 performance with 2-layer architecture

# Data Configuration
data:
  train_file: "data/train.jsonl"
  val_file: "data/val.jsonl"
  test_file: "data/test.jsonl"

# Model Architecture - Optimized for 2-layer performance
model:
  hidden_dim: 256      # Proven sufficient capacity
  num_layers: 2        # Deeper architecture for complex patterns
  layer_dropout: 0.4   # ⬆️ Increased from 0.3 for better regularization between layers
  num_classes: 2
  dropout: 0.15        # ⬇️ Reduced from 0.2 (complementing layer_dropout)

# Training Parameters - Tuned for 2-layer optimization
training:
  batch_size: 24           # Keep proven batch size
  learning_rate: 0.0006    # ⬇️ Reduced from 0.0008 for more stable 2-layer training
  weight_decay: 0.012      # ⬆️ Increased from 0.008 for stronger regularization
  max_epochs: 100          # Keep proven epoch budget
  patience: 18             # ⬆️ Increased from 12 for deeper model convergence
  gradient_clip_norm: 0.3  # ⬇️ Tighter than 0.4 for 2-layer stability
  
  # Learning rate scheduling - Keep proven cosine with adjusted parameters
  scheduler: "cosine"      # Keep proven scheduler
  min_lr: 1e-6            # Keep proven minimum
  cosine_t_max: 100       # Match max_epochs

# Anti-Collapse System - Adjusted for 2-layer dynamics
anti_collapse:
  label_smoothing: 0.18    # ⬇️ Slightly reduced from 0.20 (more regularization from layers)
  weighted_sampling: true  # Keep essential balanced sampling
  entropy_lambda: 0.05     # Keep proven entropy regularization

# Emergency Monitoring - Enhanced for 2-layer behavior
emergency_monitoring:
  enabled: true
  
  # Batch-level - Adjusted thresholds for 2-layer confidence patterns
  max_confidence_threshold: 0.92    # ⬆️ Slightly higher than 0.90 (better calibrated)
  min_chorus_rate: 0.08            # Keep proven minimum
  max_chorus_rate: 0.80            # Keep proven maximum
  max_conf_over_95_ratio: 0.12     # ⬆️ Slightly higher than 0.10 (deeper model)
  
  # Epoch-level - Adjusted for 2-layer learning dynamics
  val_overconf_threshold: 0.96     # ⬆️ Slightly higher than 0.95
  val_f1_collapse_threshold: 0.10  # ⬆️ Higher than 0.08 (more sensitive)
  emergency_overconf_threshold: 0.97  # ⬆️ Higher than 0.96
  emergency_conf95_ratio: 0.85       # ⬆️ Slightly higher than 0.80
  emergency_f1_threshold: 0.04       # ⬆️ Higher than 0.03
  
  # Monitoring timing - Keep proven monitoring schedule
  skip_batches: 30        # Keep proven early monitoring
  skip_epochs: 3          # Keep proven epoch skip
  print_batch_every: 10   # Keep proven print frequency

# Temperature Calibration - Adjusted for 2-layer confidence
temperature_calibration:
  temperature_grid: [0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 1.8, 2.2, 2.5]  # Keep proven range
  default_temperature: 0.8  # ⬆️ Higher than 0.6 for better calibration

validation_strategy: "composite"  # Keep proven validation strategy

# Feature Configuration - Keep proven feature extraction
features:
  # Head-SSM: Captures opening patterns
  head_ssm:
    enabled: true
    dimension: 12
    head_words: 2          # Keep proven setting
  
  # Tail-SSM: Captures ending patterns
  tail_ssm:
    enabled: true
    dimension: 12
    tail_words: 2          # Keep proven setting
  
  # Phonetic-SSM: Critical for chorus detection
  phonetic_ssm:
    enabled: true
    dimension: 12
    mode: "rhyme"          # Keep proven mode
    similarity_method: "binary"
    normalize: false
    normalize_method: "zscore"
    high_sim_threshold: 0.35  # Keep proven threshold
  
  # POS-SSM: Grammatical structure
  pos_ssm:
    enabled: true
    dimension: 12
    tagset: "simplified"
    similarity_method: "combined"
    high_sim_threshold: 0.28  # Keep proven threshold
  
  # String-SSM: Direct repetition detection
  string_ssm:
    enabled: true
    dimension: 12
    case_sensitive: false
    remove_punctuation: true
    similarity_threshold: 0.08  # Keep proven threshold
    similarity_method: "word_overlap"

# Output Configuration
output:
  base_dir: "training_sessions"
  save_best_model: true
  save_final_model: true
  save_training_metrics: true
  save_config_snapshot: true

# System Settings
system:
  seed: 42
  device: "auto"
  num_workers: 0          # Keep single-threaded stability
  deterministic: true

# Experiment Metadata
experiment:
  name: "training_validation_2layer_optimized_v1"
  description: "Optimized 2-layer BiLSTM configuration based on successful 0.79+ F1 performance"
  tags: ["2-layer-bilstm", "optimized", "validation-focused", "enhanced-monitoring", "production-ready"]
  notes: "Configuration tuned specifically for 2-layer architecture with adjusted learning rate, regularization, and monitoring parameters. Based on successful training reaching 0.79+ F1 score."
